{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48e9ef67-908d-458c-8ca3-0f263902a3ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Declarative Pipelines in Databricks (Delta Live Tables)\n",
    "\n",
    "In Databricks, declarative pipelines are implemented using **Delta Live Tables (DLT)**.\n",
    "\n",
    "DLT lets you define pipeline steps as **tables** (using SQL or Python). Instead of manually orchestrating tasks like “run job A, then job B, then job C”, you **declare the desired tables and transformations**, and DLT manages execution details such as ordering, retries, and monitoring.\n",
    "\n",
    "> You declare **what** you want the tables to look like — DLT manages **how** they run as a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bb952cc-785b-48e3-b1e3-c9364914e572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Core idea: Tables define the pipeline graph (DAG)\n",
    "\n",
    "In DLT, each table is a node in the pipeline.  \n",
    "If table B reads table A, then **B depends on A**.\n",
    "\n",
    "DLT automatically builds the dependency graph and runs steps in the correct order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ffcf927-1bf2-49f7-9fa7-28e08f584cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Benefits of DLT (Declarative Pipelines)\n",
    "\n",
    "#### 1) Built-in data quality checks (Expectations)\n",
    "You can define rules like “this column should never be NULL.”  \n",
    "DLT can:\n",
    "- **record** the rule outcome (monitoring),\n",
    "- **drop bad records**, or\n",
    "- **fail the pipeline** depending on how you configure the expectation.\n",
    "\n",
    "#### 2) Automatic dependency management\n",
    "You don’t manually create a DAG of tasks.  \n",
    "DLT derives the DAG from table dependencies and schedules execution accordingly.\n",
    "\n",
    "#### 3) Incremental processing support (including CDC patterns)\n",
    "DLT is designed to efficiently process new/changed data.  \n",
    "For CDC-style updates, you typically define:\n",
    "- primary keys (e.g., `order_id` or `item_id`)\n",
    "- a sequencing column (e.g., `updated_at`)\n",
    "and DLT can apply changes and maintain history (e.g., SCD Type 2) using supported CDC utilities.\n",
    "\n",
    "#### 4) Unified batch and streaming pipelines\n",
    "DLT supports both batch and streaming-style pipelines in one framework, while managing operational details like checkpoints, retries, and observability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d7cdbe4-f361-4a5d-a61a-4ca7f09f6022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Expectations (Data Quality Rules)\n",
    "\n",
    "Expectations are simple rules applied to your DLT tables.\n",
    "\n",
    "Common behaviors:\n",
    "- **expect**: track metrics, but keep records\n",
    "- **expect_or_drop**: drop records that fail the rule\n",
    "- **expect_or_fail**: fail the pipeline if rule fails\n",
    "\n",
    "Example rules:\n",
    "- `order_id IS NOT NULL`\n",
    "- `amount >= 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16eaa7a3-27de-49d2-a50e-ef27b1693e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Understanding the Databricks UI (high-level setup)\n",
    "\n",
    "#### Step 1: Create a Catalog\n",
    "- Go to **Catalog Explorer** → **Create catalog**\n",
    "- Name it (example): `dlt_plaidt`\n",
    "- Configure access (e.g., grant privileges to required users/groups)\n",
    "\n",
    "#### Step 2: Create a Schema for source data\n",
    "- In the catalog, create a schema named: `source`\n",
    "\n",
    "#### Step 3: Create and populate a source table (SQL Editor)\n",
    "- Open **SQL Editor**\n",
    "- Set catalog = `dlt_plaidt`, schema = `source`\n",
    "- Create a table (example: `orders`) and insert sample rows\n",
    "- Optional: save the query as `source_orders`\n",
    "\n",
    "#### Step 4: Organize workspace assets\n",
    "- Create a workspace folder (example: `Declarative Pipelines`)\n",
    "- Move your SQL queries/notebooks into that folder for clean organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe891d7-bae9-44ea-8983-5e3d4f866515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a Delta Live Tables Pipeline (DLT)\n",
    "\n",
    "1. Go to **Jobs & Pipelines** → **Create** → **ETL pipeline**\n",
    "2. Choose a starting point:\n",
    "   - Sample code (SQL/Python) for learning\n",
    "   - Empty pipeline for building from scratch\n",
    "   - Add existing assets if you already have DLT source files ready\n",
    "\n",
    "#### Pipeline assets (important concept)\n",
    "DLT pipelines run code from a configured **source code directory**.\n",
    "- Only files inside the declared **source code** folder are executed as part of the pipeline\n",
    "- Keep notebooks/scripts for exploration or testing **outside** the source folder\n",
    "\n",
    "Common pattern:\n",
    "- `transformations/` → pipeline source code\n",
    "- `explorations/` → EDA notebooks (not part of pipeline)\n",
    "- `utilities/` → helper modules/functions"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}