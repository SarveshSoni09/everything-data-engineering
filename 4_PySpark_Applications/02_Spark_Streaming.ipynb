{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eddc070a-96c4-4101-a42e-a787ce644784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Structured Streaming for Incremental Load (File-based Ingestion)\n",
    "\n",
    "### Scenario\n",
    "A retail company receives daily sales transaction files from multiple store locations into a cloud folder (e.g., Azure Data Lake / Unity Catalog Volume).\n",
    "\n",
    "Instead of reprocessing all historical files every day, we use **Spark Structured Streaming** to:\n",
    "- Automatically detect **newly arrived files**\n",
    "- Ingest them incrementally into a **Delta table**\n",
    "- Keep analytics tables/dashboards up-to-date while reducing compute and processing time\n",
    "\n",
    "### What we are building\n",
    "Source (landing folder with new CSV files) ➜ Structured Streaming ➜ Delta sink path (data)  \n",
    "+ a checkpoint directory to track progress (exactly-once semantics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc5b4de-bd0b-4a3b-98d3-dc988566ef25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paths used in this example\n",
    "\n",
    "**Source (new files arrive here):**\n",
    "`/Volumes/pyspark_cata/source/db_volume/spark_stream/`\n",
    "\n",
    "**Delta sink (output table stored as files):**\n",
    "`/Volumes/pyspark_cata/source/db_volume/stream_sink/data/`\n",
    "\n",
    "**Checkpoint (stream state & progress):**\n",
    "`/Volumes/pyspark_cata/source/db_volume/stream_sink/checkpoint/`\n",
    "\n",
    "> The checkpoint is mandatory for reliable streaming. It stores offsets and metadata so we don’t reprocess the same files again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d728d1a4-fd91-467f-ac42-87529672a149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define a consistent schema (recommended for CSV streams)\n",
    "\n",
    "For file-based streaming, Spark needs a stable schema to parse incoming CSV files.\n",
    "Defining it explicitly helps avoid inference issues and type mismatches as new files arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1a1e88-34c1-41be-b33c-db5357020e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_schema = \"\"\"\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    order_date DATE,\n",
    "    amount DOUBLE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca07ce6c-e6d4-4ea9-a984-1d45516f67bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Quick batch read (sanity check)\n",
    "\n",
    "Before starting the stream, we do a one-time batch read from the same source folder to:\n",
    "- confirm schema correctness\n",
    "- confirm the folder path and file format\n",
    "- preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b431a6-aaa9-4358-8365-cc5b93c23f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer_id</th><th>order_date</th><th>amount</th></tr></thead><tbody><tr><td>1</td><td>101</td><td>2025-08-02</td><td>246.84</td></tr><tr><td>2</td><td>104</td><td>2025-08-03</td><td>111.3</td></tr><tr><td>3</td><td>103</td><td>2025-08-04</td><td>52.0</td></tr><tr><td>4</td><td>103</td><td>2025-08-05</td><td>98.7</td></tr><tr><td>5</td><td>102</td><td>2025-08-06</td><td>392.67</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         101,
         "2025-08-02",
         246.84
        ],
        [
         2,
         104,
         "2025-08-03",
         111.3
        ],
        [
         3,
         103,
         "2025-08-04",
         52.0
        ],
        [
         4,
         103,
         "2025-08-05",
         98.7
        ],
        [
         5,
         102,
         "2025-08-06",
         392.67
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_batch = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .schema(stream_schema)\\\n",
    "    .load(\"/Volumes/pyspark_cata/source/db_volume/spark_stream/\")\n",
    "# Batch read = quick validation step (schema + path + preview)\n",
    "display(df_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a54bec-b61d-4d68-8db4-0c3347675503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Start streaming read from the landing folder\n",
    "\n",
    "Structured Streaming will treat new files arriving in the folder as a continuous stream.\n",
    "Each micro-batch processes only files that have not been processed yet (tracked by checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68d8e39-3cc0-4f1b-8f5e-cf6b12aa4082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .schema(stream_schema)\\\n",
    "    .load(\"/Volumes/pyspark_cata/source/db_volume/spark_stream/\")\n",
    "# Streaming read = continuously watches the folder for newly arrived files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd91e174-648d-4ed0-97c0-a9216176b06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write the stream into a Delta sink\n",
    "\n",
    "We write streaming output to a Delta path and configure:\n",
    "- **checkpointLocation**: stream progress/state (required)\n",
    "- **mergeSchema**: allow schema evolution if new columns appear (use carefully)\n",
    "- **trigger(once=True)**: runs one micro-batch and stops (good for scheduled incremental loads)\n",
    "\n",
    "> `trigger(once=True)` makes streaming behave like an incremental batch job.\n",
    "> If you want continuous ingestion, you would remove this and use a processing-time trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e2dd90-e20c-49df-9409-1fd0de3f69ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0xfffe6d536b70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/pyspark_cata/source/db_volume/stream_sink/checkpoint\")\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .trigger(once=True)\\\n",
    "    .start(\"/Volumes/pyspark_cata/source/db_volume/stream_sink/data\")\n",
    "# checkpointLocation tracks processed files (prevents duplicates on reruns)\n",
    "# trigger(once=True) makes this run like an incremental batch job (process new files, then stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99a13783-cd59-4db8-87fa-616428a1d220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify the Delta sink\n",
    "\n",
    "We query the Delta path directly to confirm that the stream wrote records successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7736255-bfb8-439e-b1c6-a3cc0c656318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer_id</th><th>order_date</th><th>amount</th></tr></thead><tbody><tr><td>1</td><td>101</td><td>2025-08-02</td><td>246.84</td></tr><tr><td>2</td><td>104</td><td>2025-08-03</td><td>111.3</td></tr><tr><td>3</td><td>103</td><td>2025-08-04</td><td>52.0</td></tr><tr><td>4</td><td>103</td><td>2025-08-05</td><td>98.7</td></tr><tr><td>5</td><td>102</td><td>2025-08-06</td><td>392.67</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         101,
         "2025-08-02",
         246.84
        ],
        [
         2,
         104,
         "2025-08-03",
         111.3
        ],
        [
         3,
         103,
         "2025-08-04",
         52.0
        ],
        [
         4,
         103,
         "2025-08-05",
         98.7
        ],
        [
         5,
         102,
         "2025-08-06",
         392.67
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "order_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "customer_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "order_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "amount",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 15
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM delta.`/Volumes/pyspark_cata/source/db_volume/stream_sink/data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "729b482f-9ae4-4e94-a82c-429788fd3913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What happens on rerun?\n",
    "\n",
    "If you run the stream again with the same checkpoint:\n",
    "- Spark will **not reprocess** already ingested files\n",
    "- Only **new files** added to the source folder will be picked up\n",
    "\n",
    "This is the main benefit of using checkpointing for file-based incremental ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24816d79-3cad-43a5-b3c5-35b810bd879a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0xfffe6d50c890>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/pyspark_cata/source/db_volume/stream_sink/checkpoint\")\\\n",
    "        .option(\"mergeSchema\", \"true\")\\\n",
    "            .trigger(once=True)\\\n",
    "                .start(\"/Volumes/pyspark_cata/source/db_volume/stream_sink/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43219fa5-034b-4564-a633-99a071f49b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer_id</th><th>order_date</th><th>amount</th></tr></thead><tbody><tr><td>6</td><td>100</td><td>2025-08-07</td><td>248.69</td></tr><tr><td>7</td><td>102</td><td>2025-08-08</td><td>243.85</td></tr><tr><td>8</td><td>101</td><td>2025-08-09</td><td>308.31</td></tr><tr><td>9</td><td>105</td><td>2025-08-10</td><td>367.45</td></tr><tr><td>10</td><td>105</td><td>2025-08-11</td><td>328.2</td></tr><tr><td>1</td><td>101</td><td>2025-08-02</td><td>246.84</td></tr><tr><td>2</td><td>104</td><td>2025-08-03</td><td>111.3</td></tr><tr><td>3</td><td>103</td><td>2025-08-04</td><td>52.0</td></tr><tr><td>4</td><td>103</td><td>2025-08-05</td><td>98.7</td></tr><tr><td>5</td><td>102</td><td>2025-08-06</td><td>392.67</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6,
         100,
         "2025-08-07",
         248.69
        ],
        [
         7,
         102,
         "2025-08-08",
         243.85
        ],
        [
         8,
         101,
         "2025-08-09",
         308.31
        ],
        [
         9,
         105,
         "2025-08-10",
         367.45
        ],
        [
         10,
         105,
         "2025-08-11",
         328.2
        ],
        [
         1,
         101,
         "2025-08-02",
         246.84
        ],
        [
         2,
         104,
         "2025-08-03",
         111.3
        ],
        [
         3,
         103,
         "2025-08-04",
         52.0
        ],
        [
         4,
         103,
         "2025-08-05",
         98.7
        ],
        [
         5,
         102,
         "2025-08-06",
         392.67
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "order_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "customer_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "order_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "amount",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 24
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM delta.`/Volumes/pyspark_cata/source/db_volume/stream_sink/data/`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8095970278048204,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Spark_Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}